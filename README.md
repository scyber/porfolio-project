# **Eventâ€‘Driven Personalization Platform**

A productionâ€‘grade, eventâ€‘driven system for realâ€‘time user personalization.  
The platform ingests user events, processes them through a streaming pipeline (Apache Flink + Kafka), builds dynamic user profiles, and exposes personalized recommendations via a REST API.

This project demonstrates **Senior Backend/Data Engineering** skills:  
distributed systems, streaming architectures, stateful processing, scalable APIs, and clean modular design.

---

# ğŸš€ **Architecture Overview**

The system consists of five major layers:

### **1. Ingestion Layer**

Receives raw user events from clients and publishes them to Kafka.

### **2. Event Backbone (Kafka)**

Acts as the central event bus with three topics:

- `raw-user-events`
- `enriched-events`
- `user-profile-updates`

### **3. Stream Processing Layer (Apache Flink)**

Three independent jobs:

- **Event Enricher** â†’ enriches raw events
- **User Profile Aggregator** â†’ maintains perâ€‘user state
- **Profile Writer** â†’ persists profiles to Redis/Postgres

### **4. Profile Store**

- **Redis** â†’ fast access for API
- **PostgreSQL** â†’ persistent storage

### **5. Personalization API**

Reads user profiles and returns personalized recommendations.

---

# ğŸ§© **Highâ€‘Level Architecture Diagram**

```plantuml
@startuml
title Event-Driven Personalization Platform - High Level Architecture

skinparam componentStyle rectangle

actor "Web App" as Web
actor "Mobile App" as Mobile

package "Ingestion Layer" {
  component "Event Ingestion Service" as Ingestion
}

node "Kafka Cluster" as Kafka {
  queue "raw-user-events" as Raw
  queue "enriched-events" as Enriched
  queue "user-profile-updates" as ProfileUpd
}

package "Stream Processing (Flink)" {
  component "Event Enricher" as Enricher
  component "User Profile Aggregator" as Aggregator
  component "Profile Writer" as Writer
}

package "User Profile Store" {
  database "PostgreSQL" as Postgres
  collections "Redis Cache" as Redis
}

package "API Layer" {
  component "Personalization API" as API
  component "Recommendation Engine" as Reco
}

Web --> Ingestion : POST /events
Mobile --> Ingestion : POST /events

Ingestion --> Raw : produce UserEvent

Raw --> Enricher : consume
Enricher --> Enriched : produce

Enriched --> Aggregator : consume
Aggregator --> ProfileUpd : produce

ProfileUpd --> Writer : consume
Writer --> Redis : update profile
Writer --> Postgres : update profile

Web --> API : GET /personalize
Mobile --> API : GET /personalize

API --> Redis : read profile
API --> Reco : build recommendations

@enduml
```

---

# ğŸ“¡ **Event Flow (Endâ€‘toâ€‘End)**

1. Client sends `UserEvent` â†’ Ingestion Service
2. Ingestion publishes to Kafka (`raw-user-events`)
3. Flink Enricher â†’ produces `EnrichedEvent`
4. Flink Aggregator â†’ updates perâ€‘user state â†’ emits `UserProfileUpdated`
5. Flink Writer â†’ stores profile in Redis + Postgres
6. API reads profile â†’ returns recommendations

---

# ğŸ§¬ **Data Model**

### **UserEvent**

```json
{
  "eventId": "uuid",
  "userId": "string",
  "eventType": "VIEW",
  "timestamp": 1714060800000,
  "context": {
    "page": "home",
    "itemId": "item001",
    "category": "TECH",
    "device": "MOBILE",
    "geo": "USA"
  }
}
```

### **EnrichedEvent**

```json
{
  "eventId": "uuid",
  "userId": "string",
  "eventType": "VIEW",
  "timestamp": 1714060800000,
  "context": {...},
  "enrichment": {
    "categoryVector": ["TECH"],
    "deviceType": "MOBILE",
    "geoRegion": "APAC"
  }
}
```

### **UserProfileUpdated**

```json
{
  "userId": "user123",
  "updatedAt": 1714060800000,
  "profile": {
    "interests": { "tech": 3.0 },
    "recentItems": ["item001"],
    "activityScore": 0.3
  }
}
```

---

# ğŸ§± **Project Structure (Maven Multiâ€‘Module)**

```
platform-parent/
â”œâ”€ core-model/
â”œâ”€ ingestion-service/
â”œâ”€ streaming-event-enricher/
â”œâ”€ streaming-profile-aggregator/
â”œâ”€ streaming-profile-writer/
â”œâ”€ personalization-api/
â”œâ”€ infra/
â””â”€ docs/
```

Each module is fully isolated and productionâ€‘ready.

---

# ğŸ³ **Local Development (Docker)**

Start Kafka, Redis, Postgres:

```
cd infra
docker-compose up -d
```

---

# ğŸ› ï¸ **Build All Modules**

```
mvn clean install -DskipTests
```

---

# â–¶ï¸ **Run Services**

### Ingestion Service

```
cd ingestion-service
mvn spring-boot:run
```

### Personalization API

```
cd personalization-api
mvn spring-boot:run
```

### Flink Jobs

```
java -jar streaming-event-enricher/target/*.jar
java -jar streaming-profile-aggregator/target/*.jar
java -jar streaming-profile-writer/target/*.jar
```

---

# ğŸ§ª **Testing the Pipeline**

### Send a test event

```
curl -X POST http://localhost:8081/events \
  -H "Content-Type: application/json" \
  -d '{"userId":"user123","eventType":"VIEW","timestamp":1714060800000,
       "context":{"page":"home","itemId":"item001","category":"tech","device":"mobile","geo":"de"}}'
```

### Request personalization

```
curl "http://localhost:8080/api/v1/personalize?userId=user123&limit=5"
```

---

# ğŸ¯ **What This Project Demonstrates**

This platform showcases:

### **Backend Engineering**

- Clean modular architecture
- REST API design
- Redis/Postgres integration

### **Data Engineering**

- Kafka event modeling
- Stateful stream processing (Flink)
- Realâ€‘time profile aggregation

### **Distributed Systems**

- Eventâ€‘driven architecture
- Horizontal scalability
- Loose coupling between services

### **Productionâ€‘readiness**

- Dockerized infra
- Multiâ€‘module Maven build
- Clear separation of concerns

---

# ğŸ“„ **License**

MIT / Apacheâ€‘2.0 (choose one)
